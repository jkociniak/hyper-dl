{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597c516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jk394348/hyper-dl\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('/home/jk394348/hyper-dl')\n",
    "print(os.getcwd())\n",
    "\n",
    "import pickle\n",
    "from omegaconf import DictConfig\n",
    "from config import run_training\n",
    "from data import build_loaders\n",
    "from plot import setup_dirs, dump_results\n",
    "\n",
    "\n",
    "def build_dataset_path(seed, n_samples, dim, eps, curv, transform_dim):\n",
    "    template = 'dim={},n_samples={},eps={},transform_dim={},curv={},seed={}.pkl'\n",
    "    return template.format(int(dim), int(n_samples), eps, transform_dim, curv, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56234ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from omegaconf import DictConfig\n",
    "from config import run_training\n",
    "from data import build_loaders\n",
    "from plot import setup_dirs, dump_results\n",
    "\n",
    "\n",
    "def build_dataset_path(seed, n_samples, dim, eps, curv, transform_dim):\n",
    "    template = 'dim={},n_samples={},eps={},transform_dim={},curv={},seed={}.pkl'\n",
    "    return template.format(int(dim), int(n_samples), eps, transform_dim, curv, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd9c373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.utils import get_original_cwd\n",
    "from hydra import initialize, compose\n",
    "\n",
    "overrides = [\n",
    "    'num_workers=2',\n",
    "    'dataset_params.dim=1',\n",
    "    #'~neptune_cfg',\n",
    "    'epochs=400',\n",
    "    'model=relu_d5_w320',\n",
    "    #'scheduler.name=None',\n",
    "    # 'model/head=hyperbolic_dist',\n",
    "    # 'model.encoder.hidden_dims=[32,32]',\n",
    "    # 'model.encoder.bias=True',\n",
    "    'bs=4096',\n",
    "    'optimizer.lr=0.016',\n",
    "    'r_optimizer.lr=0.008'\n",
    "]\n",
    "\n",
    "with initialize(config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config.yaml\", overrides=overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2425db39-85c8-4a3a-8703-ee7126bc336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory : /home/jk394348/hyper-dl\n",
      "https://app.neptune.ai/jkociniak/hyperbolic-dist-pred/e/HYPERDIST-2760\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Found device: cuda\n",
      "[1] train loss (mean of batch losses): 3.1903996372767858\n",
      "[1] val loss: 2.0795427734375\n",
      "[2] train loss (mean of batch losses): 1.224188387625558\n",
      "[2] val loss: 0.557686376953125\n",
      "[3] train loss (mean of batch losses): 0.454265274483817\n",
      "[3] val loss: 0.35017421264648435\n",
      "[4] train loss (mean of batch losses): 0.3067190115792411\n",
      "[4] val loss: 0.24490597534179687\n",
      "[5] train loss (mean of batch losses): 0.1974021741594587\n",
      "[5] val loss: 0.15146104431152344\n",
      "[6] train loss (mean of batch losses): 0.12732552577427456\n",
      "[6] val loss: 0.10561213989257813\n",
      "[7] train loss (mean of batch losses): 0.08745457720075335\n",
      "[7] val loss: 0.07392281799316407\n",
      "[8] train loss (mean of batch losses): 0.059561073303222654\n",
      "[8] val loss: 0.05030989761352539\n",
      "[9] train loss (mean of batch losses): 0.04325851549421038\n",
      "[9] val loss: 0.03891150131225586\n",
      "[10] train loss (mean of batch losses): 0.03207762407575335\n",
      "[10] val loss: 0.02539745635986328\n",
      "[11] train loss (mean of batch losses): 0.055680216217041015\n",
      "[11] val loss: 0.07302260284423828\n",
      "[12] train loss (mean of batch losses): 0.08833583047049386\n",
      "[12] val loss: 0.0406343391418457\n",
      "[13] train loss (mean of batch losses): 0.035029044342041016\n",
      "[13] val loss: 0.023531150436401367\n",
      "[14] train loss (mean of batch losses): 0.018454419544764927\n",
      "[14] val loss: 0.023338699722290038\n",
      "[15] train loss (mean of batch losses): 0.03063428758893694\n",
      "[15] val loss: 0.020053403854370117\n",
      "[16] train loss (mean of batch losses): 0.01884242101396833\n",
      "[16] val loss: 0.020748675155639648\n",
      "[17] train loss (mean of batch losses): 0.023574912425449915\n",
      "[17] val loss: 0.023266015625\n",
      "[18] train loss (mean of batch losses): 0.05967938390459333\n",
      "[18] val loss: 0.24141076049804688\n",
      "[19] train loss (mean of batch losses): 0.12268170994349889\n",
      "[19] val loss: 0.06718140411376954\n",
      "[20] train loss (mean of batch losses): 0.041277622549874445\n",
      "[20] val loss: 0.03172339897155762\n",
      "[21] train loss (mean of batch losses): 0.023854475729806084\n",
      "[21] val loss: 0.019439145660400392\n",
      "[22] train loss (mean of batch losses): 0.016555379704066685\n",
      "[22] val loss: 0.015553080558776855\n",
      "[23] train loss (mean of batch losses): 0.013012858200073241\n",
      "[23] val loss: 0.015071213150024415\n",
      "[24] train loss (mean of batch losses): 0.01620003068106515\n",
      "[24] val loss: 0.013780869102478027\n",
      "[25] train loss (mean of batch losses): 0.01629521882193429\n",
      "[25] val loss: 0.01439209499359131\n",
      "[26] train loss (mean of batch losses): 0.0241798216683524\n",
      "[26] val loss: 0.020398821258544923\n",
      "[27] train loss (mean of batch losses): 0.010658681460789271\n",
      "[27] val loss: 0.007751941776275635\n",
      "[28] train loss (mean of batch losses): 0.0063091489791870115\n",
      "[28] val loss: 0.008769581031799317\n",
      "[29] train loss (mean of batch losses): 0.006841533497401646\n",
      "[29] val loss: 0.0056303180694580075\n",
      "[30] train loss (mean of batch losses): 0.008869772774832588\n",
      "[30] val loss: 0.028144703674316405\n",
      "[31] train loss (mean of batch losses): 0.03923314127240862\n",
      "[31] val loss: 0.05066716156005859\n",
      "[32] train loss (mean of batch losses): 0.024722225734165736\n",
      "[32] val loss: 0.01939910469055176\n",
      "[33] train loss (mean of batch losses): 0.012515750776018415\n",
      "[33] val loss: 0.009326531219482423\n",
      "[34] train loss (mean of batch losses): 0.007644687938690185\n",
      "[34] val loss: 0.007531716251373291\n",
      "[35] train loss (mean of batch losses): 0.004814950016566686\n",
      "[35] val loss: 0.005522828769683838\n",
      "[36] train loss (mean of batch losses): 0.0043305336407252724\n",
      "[36] val loss: 0.006535054492950439\n",
      "[37] train loss (mean of batch losses): 0.00977874140058245\n",
      "[37] val loss: 0.01746566047668457\n",
      "[38] train loss (mean of batch losses): 0.005275940309252058\n",
      "[38] val loss: 0.009590811729431152\n",
      "[39] train loss (mean of batch losses): 0.01004707144328526\n",
      "[39] val loss: 0.018404512023925783\n",
      "[40] train loss (mean of batch losses): 0.051458626501900806\n",
      "[40] val loss: 0.04966006546020508\n",
      "[41] train loss (mean of batch losses): 0.03436840275355748\n",
      "[41] val loss: 0.022498880767822266\n",
      "[42] train loss (mean of batch losses): 0.015991769518171037\n",
      "[42] val loss: 0.007615200805664062\n",
      "[43] train loss (mean of batch losses): 0.008051925550188337\n",
      "[43] val loss: 0.00799227819442749\n",
      "[44] train loss (mean of batch losses): 0.00528826505116054\n",
      "[44] val loss: 0.005185319137573243\n",
      "[45] train loss (mean of batch losses): 0.0069496524061475485\n",
      "[45] val loss: 0.004863633060455322\n",
      "[46] train loss (mean of batch losses): 0.011832127026149206\n",
      "[46] val loss: 0.02136803550720215\n",
      "[47] train loss (mean of batch losses): 0.026223987088884626\n",
      "[47] val loss: 0.01539457893371582\n",
      "[48] train loss (mean of batch losses): 0.008516633524213519\n",
      "[48] val loss: 0.009667182350158692\n",
      "[49] train loss (mean of batch losses): 0.006305540030343192\n",
      "[49] val loss: 0.009721695709228515\n",
      "[50] train loss (mean of batch losses): 0.006103870051247733\n",
      "[50] val loss: 0.004902765560150147\n",
      "[51] train loss (mean of batch losses): 0.005193368366786411\n",
      "[51] val loss: 0.005632242679595947\n",
      "[52] train loss (mean of batch losses): 0.007598799623761858\n",
      "[52] val loss: 0.0065110745429992675\n",
      "[53] train loss (mean of batch losses): 0.007112524632045201\n",
      "[53] val loss: 0.02868246955871582\n",
      "[54] train loss (mean of batch losses): 0.00817648503439767\n",
      "[54] val loss: 0.008140614700317383\n",
      "[55] train loss (mean of batch losses): 0.012829439680916922\n",
      "[55] val loss: 0.10721167602539063\n",
      "[56] train loss (mean of batch losses): 0.05360160391671317\n",
      "[56] val loss: 0.02066726837158203\n",
      "Epoch    56: reducing learning rate of group 0 to 1.2000e-02.\n",
      "[57] train loss (mean of batch losses): 0.015917100361415318\n",
      "[57] val loss: 0.00845747709274292\n",
      "[58] train loss (mean of batch losses): 0.007270561408996582\n",
      "[58] val loss: 0.005175188446044922\n",
      "[59] train loss (mean of batch losses): 0.0043501006671360564\n",
      "[59] val loss: 0.005820016670227051\n",
      "[60] train loss (mean of batch losses): 0.0037989610263279507\n",
      "[60] val loss: 0.003452064561843872\n",
      "[61] train loss (mean of batch losses): 0.0029767950194222585\n",
      "[61] val loss: 0.0034769747257232665\n",
      "[62] train loss (mean of batch losses): 0.002749526350838797\n",
      "[62] val loss: 0.0025757015228271484\n",
      "[63] train loss (mean of batch losses): 0.002577281529562814\n",
      "[63] val loss: 0.0033916030406951903\n",
      "[64] train loss (mean of batch losses): 0.00239873183795384\n",
      "[64] val loss: 0.0019663445234298707\n",
      "[65] train loss (mean of batch losses): 0.0017825723103114537\n",
      "[65] val loss: 0.0019069464921951293\n",
      "[66] train loss (mean of batch losses): 0.0017981489794594901\n",
      "[66] val loss: 0.0019232491970062255\n",
      "[67] train loss (mean of batch losses): 0.0017473620755331857\n",
      "[67] val loss: 0.0032820825099945067\n",
      "[68] train loss (mean of batch losses): 0.0034853171621050155\n",
      "[68] val loss: 0.0021289745807647705\n",
      "[69] train loss (mean of batch losses): 0.002547989845275879\n",
      "[69] val loss: 0.007706892681121826\n",
      "[70] train loss (mean of batch losses): 0.003382953527995518\n",
      "[70] val loss: 0.0078834077835083\n",
      "[71] train loss (mean of batch losses): 0.003121613325391497\n",
      "[71] val loss: 0.002101020860671997\n",
      "[72] train loss (mean of batch losses): 0.001436232692854745\n",
      "[72] val loss: 0.0020192517518997193\n",
      "[73] train loss (mean of batch losses): 0.00147944518498012\n",
      "[73] val loss: 0.0016856814861297606\n",
      "[74] train loss (mean of batch losses): 0.0018575997829437256\n",
      "[74] val loss: 0.003398004102706909\n",
      "[75] train loss (mean of batch losses): 0.002104237004688808\n",
      "[75] val loss: 0.00193559730052948\n",
      "[76] train loss (mean of batch losses): 0.0014678034101213728\n",
      "[76] val loss: 0.001678494119644165\n",
      "[77] train loss (mean of batch losses): 0.0014320133890424456\n",
      "[77] val loss: 0.0014315261840820313\n",
      "[78] train loss (mean of batch losses): 0.001514185871396746\n",
      "[78] val loss: 0.002741597509384155\n",
      "[79] train loss (mean of batch losses): 0.0016329373223440988\n",
      "[79] val loss: 0.0026832653045654295\n",
      "[80] train loss (mean of batch losses): 0.0014012098550796508\n",
      "[80] val loss: 0.0017861268281936645\n",
      "[81] train loss (mean of batch losses): 0.0012622455903462\n",
      "[81] val loss: 0.0012040130376815795\n",
      "[82] train loss (mean of batch losses): 0.0012462406941822597\n",
      "[82] val loss: 0.001562330436706543\n",
      "[83] train loss (mean of batch losses): 0.001223184050832476\n",
      "[83] val loss: 0.0023966196537017824\n",
      "[84] train loss (mean of batch losses): 0.0013477624416351319\n",
      "[84] val loss: 0.0013135777473449706\n",
      "[85] train loss (mean of batch losses): 0.0012480365719114031\n",
      "[85] val loss: 0.001211162495613098\n",
      "[86] train loss (mean of batch losses): 0.0011153602804456438\n",
      "[86] val loss: 0.0012179922342300416\n",
      "[87] train loss (mean of batch losses): 0.0015960041727338518\n",
      "[87] val loss: 0.0017604126453399658\n",
      "[88] train loss (mean of batch losses): 0.0024336050271987915\n",
      "[88] val loss: 0.0015610783815383912\n",
      "[89] train loss (mean of batch losses): 0.0016406137841088431\n",
      "[89] val loss: 0.0017532085657119751\n",
      "[90] train loss (mean of batch losses): 0.0014666221516472953\n",
      "[90] val loss: 0.0014496955394744874\n",
      "[91] train loss (mean of batch losses): 0.0012764708927699498\n",
      "[91] val loss: 0.0011408922910690308\n",
      "[92] train loss (mean of batch losses): 0.001135187884739467\n",
      "[92] val loss: 0.0010162432551383972\n",
      "[93] train loss (mean of batch losses): 0.0019118154730115619\n",
      "[93] val loss: 0.0012238110065460204\n",
      "[94] train loss (mean of batch losses): 0.0010035883154187884\n",
      "[94] val loss: 0.0011664141416549683\n",
      "[95] train loss (mean of batch losses): 0.0010332402501787457\n",
      "[95] val loss: 0.002341291570663452\n",
      "[96] train loss (mean of batch losses): 0.0016033158608845302\n",
      "[96] val loss: 0.0012306609153747558\n",
      "[97] train loss (mean of batch losses): 0.000925618382862636\n",
      "[97] val loss: 0.0010238579630851745\n",
      "[98] train loss (mean of batch losses): 0.0014217201709747313\n",
      "[98] val loss: 0.0027639936447143554\n",
      "[99] train loss (mean of batch losses): 0.0016933621849332537\n",
      "[99] val loss: 0.0014333022832870483\n",
      "[100] train loss (mean of batch losses): 0.0017948138577597481\n",
      "[100] val loss: 0.0012597434759140014\n",
      "[101] train loss (mean of batch losses): 0.001689903027670724\n",
      "[101] val loss: 0.0012153828144073487\n",
      "[102] train loss (mean of batch losses): 0.0009688388211386544\n",
      "[102] val loss: 0.0010670787572860717\n",
      "[103] train loss (mean of batch losses): 0.0014952463729040963\n",
      "[103] val loss: 0.0025269145488739015\n",
      "Epoch   103: reducing learning rate of group 0 to 9.0000e-03.\n",
      "[104] train loss (mean of batch losses): 0.0015753495284489224\n",
      "[104] val loss: 0.0010539441108703613\n",
      "[105] train loss (mean of batch losses): 0.000810732696737562\n",
      "[105] val loss: 0.0007653704285621643\n",
      "[106] train loss (mean of batch losses): 0.0007186990192958287\n",
      "[106] val loss: 0.0007710016965866089\n",
      "[107] train loss (mean of batch losses): 0.0006308620027133397\n",
      "[107] val loss: 0.0006907390832901001\n",
      "[108] train loss (mean of batch losses): 0.0006354983636311123\n",
      "[108] val loss: 0.0007323054075241089\n",
      "[109] train loss (mean of batch losses): 0.0006703629510743278\n",
      "[109] val loss: 0.0007833301782608032\n",
      "[110] train loss (mean of batch losses): 0.0006394695741789682\n",
      "[110] val loss: 0.0012187336206436158\n",
      "[111] train loss (mean of batch losses): 0.0007048351049423218\n",
      "[111] val loss: 0.0006525969505310059\n",
      "[112] train loss (mean of batch losses): 0.0006122550879205976\n",
      "[112] val loss: 0.0009618547439575196\n",
      "[113] train loss (mean of batch losses): 0.0006987563848495483\n",
      "[113] val loss: 0.0008283900737762451\n",
      "[114] train loss (mean of batch losses): 0.0007431567209107535\n",
      "[114] val loss: 0.000699270248413086\n",
      "[115] train loss (mean of batch losses): 0.0006271330322538103\n",
      "[115] val loss: 0.0007649445176124573\n",
      "[116] train loss (mean of batch losses): 0.0006230630363736833\n",
      "[116] val loss: 0.0008543331146240235\n",
      "[117] train loss (mean of batch losses): 0.000731663863999503\n",
      "[117] val loss: 0.0012773842096328735\n",
      "[118] train loss (mean of batch losses): 0.0007062263267380851\n",
      "[118] val loss: 0.0006887970209121704\n",
      "[119] train loss (mean of batch losses): 0.0006101145250456674\n",
      "[119] val loss: 0.0008533185601234437\n",
      "[120] train loss (mean of batch losses): 0.0006198991724423\n",
      "[120] val loss: 0.0007595489621162415\n",
      "[121] train loss (mean of batch losses): 0.000754617702960968\n",
      "[121] val loss: 0.0009061517953872681\n",
      "[122] train loss (mean of batch losses): 0.0006124331917081561\n",
      "[122] val loss: 0.0007070080876350402\n",
      "Epoch   122: reducing learning rate of group 0 to 6.7500e-03.\n",
      "[123] train loss (mean of batch losses): 0.0005419279847826276\n",
      "[123] val loss: 0.0005792049884796143\n",
      "[124] train loss (mean of batch losses): 0.0005019620946475438\n",
      "[124] val loss: 0.000677688729763031\n",
      "[125] train loss (mean of batch losses): 0.000528019574710301\n",
      "[125] val loss: 0.0006156632781028747\n",
      "[126] train loss (mean of batch losses): 0.0005145849278994969\n",
      "[126] val loss: 0.0006740916252136231\n",
      "[127] train loss (mean of batch losses): 0.0005623366747583661\n",
      "[127] val loss: 0.0006300641775131225\n",
      "[128] train loss (mean of batch losses): 0.000532365882396698\n",
      "[128] val loss: 0.0006153494834899902\n",
      "[129] train loss (mean of batch losses): 0.0005096544367926461\n",
      "[129] val loss: 0.0005902869582176209\n",
      "[130] train loss (mean of batch losses): 0.0004891715509550912\n",
      "[130] val loss: 0.000596576702594757\n",
      "[131] train loss (mean of batch losses): 0.000484080708026886\n",
      "[131] val loss: 0.0005645933866500855\n",
      "[132] train loss (mean of batch losses): 0.0005178536329950605\n",
      "[132] val loss: 0.0006157256364822388\n",
      "[133] train loss (mean of batch losses): 0.00047598052876336234\n",
      "[133] val loss: 0.0005409923195838928\n",
      "[134] train loss (mean of batch losses): 0.00046947387201445444\n",
      "[134] val loss: 0.0005850128293037414\n",
      "[135] train loss (mean of batch losses): 0.0005014784693717956\n",
      "[135] val loss: 0.0006090449213981628\n",
      "[136] train loss (mean of batch losses): 0.0005088054810251509\n",
      "[136] val loss: 0.0005218686163425446\n",
      "[137] train loss (mean of batch losses): 0.0005010992612157549\n",
      "[137] val loss: 0.0007195921540260315\n",
      "[138] train loss (mean of batch losses): 0.0005496504289763314\n",
      "[138] val loss: 0.0006382265210151672\n",
      "[139] train loss (mean of batch losses): 0.0004949490530150278\n",
      "[139] val loss: 0.0005976046085357667\n",
      "[140] train loss (mean of batch losses): 0.00048540024927684237\n",
      "[140] val loss: 0.0006858975052833557\n",
      "[141] train loss (mean of batch losses): 0.000577254455430167\n",
      "[141] val loss: 0.0005122755408287048\n",
      "[142] train loss (mean of batch losses): 0.0004668929065976824\n",
      "[142] val loss: 0.0006726703882217408\n",
      "[143] train loss (mean of batch losses): 0.0004982458574431284\n",
      "[143] val loss: 0.0005547163903713226\n",
      "[144] train loss (mean of batch losses): 0.0004786817686898368\n",
      "[144] val loss: 0.0005571459174156189\n",
      "[145] train loss (mean of batch losses): 0.00044769922750336784\n",
      "[145] val loss: 0.0005513960838317871\n",
      "[146] train loss (mean of batch losses): 0.00047106442281178064\n",
      "[146] val loss: 0.0005490501761436463\n",
      "[147] train loss (mean of batch losses): 0.0004525688392775399\n",
      "[147] val loss: 0.0005363451838493348\n",
      "[148] train loss (mean of batch losses): 0.00045908446993146626\n",
      "[148] val loss: 0.0005720836520195007\n",
      "[149] train loss (mean of batch losses): 0.0004529292208807809\n",
      "[149] val loss: 0.0006386454224586487\n",
      "[150] train loss (mean of batch losses): 0.00046014570082936966\n",
      "[150] val loss: 0.0005285939037799835\n",
      "[151] train loss (mean of batch losses): 0.0004098662052835737\n",
      "[151] val loss: 0.0005998438954353333\n",
      "[152] train loss (mean of batch losses): 0.0004538052099091666\n",
      "[152] val loss: 0.0006647877931594848\n",
      "Epoch   152: reducing learning rate of group 0 to 5.0625e-03.\n",
      "[153] train loss (mean of batch losses): 0.00043816035645348684\n",
      "[153] val loss: 0.0005439020216464997\n",
      "[154] train loss (mean of batch losses): 0.0004249811513083322\n",
      "[154] val loss: 0.0005708134412765503\n",
      "[155] train loss (mean of batch losses): 0.0004149729660579136\n",
      "[155] val loss: 0.000504749447107315\n",
      "[156] train loss (mean of batch losses): 0.0004151456321988787\n",
      "[156] val loss: 0.0004924993395805359\n",
      "[157] train loss (mean of batch losses): 0.0004027825713157654\n",
      "[157] val loss: 0.0005577860116958618\n",
      "[158] train loss (mean of batch losses): 0.0004118865132331848\n",
      "[158] val loss: 0.00046811588406562806\n",
      "[159] train loss (mean of batch losses): 0.0003806168820176806\n",
      "[159] val loss: 0.0005107050716876984\n",
      "[160] train loss (mean of batch losses): 0.00040349532195499963\n",
      "[160] val loss: 0.00046827007532119754\n",
      "[161] train loss (mean of batch losses): 0.0003820552468299866\n",
      "[161] val loss: 0.00048324053287506104\n",
      "[162] train loss (mean of batch losses): 0.00040300101722989763\n",
      "[162] val loss: 0.0004920112669467926\n",
      "[163] train loss (mean of batch losses): 0.00042949846812656946\n",
      "[163] val loss: 0.00048635359406471254\n",
      "[164] train loss (mean of batch losses): 0.00041622457844870433\n",
      "[164] val loss: 0.0005740946769714355\n",
      "[165] train loss (mean of batch losses): 0.0004202522941998073\n",
      "[165] val loss: 0.0006115408658981323\n",
      "[166] train loss (mean of batch losses): 0.00042670543193817136\n",
      "[166] val loss: 0.00047074711322784424\n",
      "[167] train loss (mean of batch losses): 0.0004095660311835153\n",
      "[167] val loss: 0.0005159782707691193\n",
      "[168] train loss (mean of batch losses): 0.0004144538027899606\n",
      "[168] val loss: 0.0004677859246730804\n",
      "[169] train loss (mean of batch losses): 0.00038877056155885967\n",
      "[169] val loss: 0.0004495662271976471\n",
      "[170] train loss (mean of batch losses): 0.00039155650309153966\n",
      "[170] val loss: 0.0004368585169315338\n",
      "[171] train loss (mean of batch losses): 0.00036244063377380373\n",
      "[171] val loss: 0.00043444809913635254\n",
      "[172] train loss (mean of batch losses): 0.0003963680761201041\n",
      "[172] val loss: 0.0005180023908615113\n",
      "[173] train loss (mean of batch losses): 0.0004247048658984048\n",
      "[173] val loss: 0.0004250564455986023\n",
      "[174] train loss (mean of batch losses): 0.00038963971648897443\n",
      "[174] val loss: 0.0005403697669506073\n",
      "[175] train loss (mean of batch losses): 0.00039874343020575385\n",
      "[175] val loss: 0.000493223923444748\n",
      "[176] train loss (mean of batch losses): 0.00036543928214481896\n",
      "[176] val loss: 0.0004966005504131317\n",
      "[177] train loss (mean of batch losses): 0.00044966792379106795\n",
      "[177] val loss: 0.0005514945983886719\n",
      "[178] train loss (mean of batch losses): 0.00042715514046805247\n",
      "[178] val loss: 0.00043529489040374756\n",
      "[179] train loss (mean of batch losses): 0.000369537398644856\n",
      "[179] val loss: 0.0004355301797389984\n",
      "[180] train loss (mean of batch losses): 0.0003503009625843593\n",
      "[180] val loss: 0.00042066748142242433\n",
      "[181] train loss (mean of batch losses): 0.0003709819504192897\n",
      "[181] val loss: 0.0005142627418041229\n",
      "[182] train loss (mean of batch losses): 0.00039189601966312955\n",
      "[182] val loss: 0.0004388540267944336\n",
      "[183] train loss (mean of batch losses): 0.00035296496323176794\n",
      "[183] val loss: 0.0004248955965042114\n",
      "[184] train loss (mean of batch losses): 0.00034769428627831595\n",
      "[184] val loss: 0.00047202861309051514\n",
      "[185] train loss (mean of batch losses): 0.0003667518786021641\n",
      "[185] val loss: 0.0003965164601802826\n",
      "[186] train loss (mean of batch losses): 0.0003717299222946167\n",
      "[186] val loss: 0.0004473339140415192\n",
      "[187] train loss (mean of batch losses): 0.00037085429089409966\n",
      "[187] val loss: 0.00041904608607292175\n",
      "[188] train loss (mean of batch losses): 0.00035451849528721403\n",
      "[188] val loss: 0.00045711159706115725\n",
      "[189] train loss (mean of batch losses): 0.000361213059084756\n",
      "[189] val loss: 0.0004473099887371063\n",
      "[190] train loss (mean of batch losses): 0.00039456273317337037\n",
      "[190] val loss: 0.0004196555495262146\n",
      "[191] train loss (mean of batch losses): 0.0003527487218379974\n",
      "[191] val loss: 0.0004229432225227356\n",
      "[192] train loss (mean of batch losses): 0.0003508238954203469\n",
      "[192] val loss: 0.00040630984902381896\n",
      "[193] train loss (mean of batch losses): 0.000326164493390492\n",
      "[193] val loss: 0.0004295718848705292\n",
      "[194] train loss (mean of batch losses): 0.000334307438986642\n",
      "[194] val loss: 0.0004909101247787476\n",
      "[195] train loss (mean of batch losses): 0.00039001778364181517\n",
      "[195] val loss: 0.0004236167311668396\n",
      "[196] train loss (mean of batch losses): 0.00032994039654731753\n",
      "[196] val loss: 0.0004681697905063629\n",
      "Epoch   196: reducing learning rate of group 0 to 3.7969e-03.\n",
      "[197] train loss (mean of batch losses): 0.00033908511996269227\n",
      "[197] val loss: 0.00039402692914009094\n",
      "[198] train loss (mean of batch losses): 0.0003373684891632625\n",
      "[198] val loss: 0.0003965725600719452\n",
      "[199] train loss (mean of batch losses): 0.00032971241303852625\n",
      "[199] val loss: 0.000411833518743515\n",
      "[200] train loss (mean of batch losses): 0.0003484349306140627\n",
      "[200] val loss: 0.0005143752455711365\n",
      "[201] train loss (mean of batch losses): 0.00035043728010995047\n",
      "[201] val loss: 0.000379813814163208\n",
      "[202] train loss (mean of batch losses): 0.0003177056533949716\n",
      "[202] val loss: 0.00038835352659225464\n",
      "[203] train loss (mean of batch losses): 0.0003193431121962411\n",
      "[203] val loss: 0.0003887672603130341\n",
      "[204] train loss (mean of batch losses): 0.0003107884611402239\n",
      "[204] val loss: 0.0003857403755187988\n",
      "[205] train loss (mean of batch losses): 0.0003118170976638794\n",
      "[205] val loss: 0.0003921744227409363\n",
      "[206] train loss (mean of batch losses): 0.00033460787279265266\n",
      "[206] val loss: 0.0004179690897464752\n",
      "[207] train loss (mean of batch losses): 0.0003250718329633985\n",
      "[207] val loss: 0.0003934701204299927\n",
      "[208] train loss (mean of batch losses): 0.00031117478098188127\n",
      "[208] val loss: 0.00040037291049957273\n",
      "[209] train loss (mean of batch losses): 0.0003063722618988582\n",
      "[209] val loss: 0.000440610659122467\n",
      "[210] train loss (mean of batch losses): 0.0003245222244943891\n",
      "[210] val loss: 0.0004426185369491577\n",
      "[211] train loss (mean of batch losses): 0.00033021473203386577\n",
      "[211] val loss: 0.0004402937114238739\n",
      "[212] train loss (mean of batch losses): 0.0003346025330679757\n",
      "[212] val loss: 0.000406632661819458\n",
      "Epoch   212: reducing learning rate of group 0 to 2.8477e-03.\n",
      "[213] train loss (mean of batch losses): 0.0003239399646009718\n",
      "[213] val loss: 0.0003722585618495941\n",
      "[214] train loss (mean of batch losses): 0.0003221171362059457\n",
      "[214] val loss: 0.00036048778891563415\n",
      "[215] train loss (mean of batch losses): 0.00032108167239597866\n",
      "[215] val loss: 0.0003785885810852051\n",
      "[216] train loss (mean of batch losses): 0.0003116822413035802\n",
      "[216] val loss: 0.00036358540654182433\n",
      "[217] train loss (mean of batch losses): 0.0003060754529067448\n",
      "[217] val loss: 0.00039484153985977174\n",
      "[218] train loss (mean of batch losses): 0.0003109273484774998\n",
      "[218] val loss: 0.00039448970556259157\n",
      "[219] train loss (mean of batch losses): 0.0003163510433265141\n",
      "[219] val loss: 0.00038761926889419556\n",
      "[220] train loss (mean of batch losses): 0.00030255533286503386\n",
      "[220] val loss: 0.00036694071292877196\n",
      "[221] train loss (mean of batch losses): 0.0003012809497969491\n",
      "[221] val loss: 0.00036490980982780455\n",
      "[222] train loss (mean of batch losses): 0.00031299346429961066\n",
      "[222] val loss: 0.0003705737829208374\n",
      "[223] train loss (mean of batch losses): 0.0003034086883068085\n",
      "[223] val loss: 0.0003669192910194397\n",
      "[224] train loss (mean of batch losses): 0.00029593538982527595\n",
      "[224] val loss: 0.0003587043821811676\n",
      "[225] train loss (mean of batch losses): 0.0002974218572889056\n",
      "[225] val loss: 0.0003517772674560547\n",
      "[226] train loss (mean of batch losses): 0.0003072724257196699\n",
      "[226] val loss: 0.000360556036233902\n",
      "[227] train loss (mean of batch losses): 0.00029903203078678677\n",
      "[227] val loss: 0.00035944888591766356\n",
      "[228] train loss (mean of batch losses): 0.00030194739358765737\n",
      "[228] val loss: 0.00035073851346969603\n",
      "[229] train loss (mean of batch losses): 0.0003065273744719369\n",
      "[229] val loss: 0.00035467894673347475\n",
      "[230] train loss (mean of batch losses): 0.00033055887818336484\n",
      "[230] val loss: 0.00043440711498260497\n",
      "[231] train loss (mean of batch losses): 0.0003274504508290972\n",
      "[231] val loss: 0.0003679345428943634\n",
      "[232] train loss (mean of batch losses): 0.0003054199618952615\n",
      "[232] val loss: 0.0003540102481842041\n",
      "[233] train loss (mean of batch losses): 0.0003079893273966653\n",
      "[233] val loss: 0.00035582004189491274\n",
      "[234] train loss (mean of batch losses): 0.00031288777589797974\n",
      "[234] val loss: 0.0003470373690128326\n",
      "[235] train loss (mean of batch losses): 0.0002828745058604649\n",
      "[235] val loss: 0.000355608195066452\n",
      "[236] train loss (mean of batch losses): 0.0003027380355766841\n",
      "[236] val loss: 0.0003510143578052521\n",
      "[237] train loss (mean of batch losses): 0.00028710502130644663\n",
      "[237] val loss: 0.0003937257289886475\n",
      "[238] train loss (mean of batch losses): 0.00030160459450313023\n",
      "[238] val loss: 0.000347622686624527\n",
      "[239] train loss (mean of batch losses): 0.00030248111060687474\n",
      "[239] val loss: 0.00035273104310035706\n",
      "[240] train loss (mean of batch losses): 0.0002924155652523041\n",
      "[240] val loss: 0.00035416595339775086\n",
      "[241] train loss (mean of batch losses): 0.0002783184434686388\n",
      "[241] val loss: 0.000346851921081543\n",
      "[242] train loss (mean of batch losses): 0.00029536874038832526\n",
      "[242] val loss: 0.0003493516683578491\n",
      "[243] train loss (mean of batch losses): 0.0002924888619354793\n",
      "[243] val loss: 0.0003440266251564026\n",
      "[244] train loss (mean of batch losses): 0.0002914284927504403\n",
      "[244] val loss: 0.0003439658224582672\n",
      "[245] train loss (mean of batch losses): 0.00028669106279100693\n",
      "[245] val loss: 0.0003464956879615784\n",
      "[246] train loss (mean of batch losses): 0.00028242952823638915\n",
      "[246] val loss: 0.00033755576014518737\n",
      "[247] train loss (mean of batch losses): 0.00028653704353741235\n",
      "[247] val loss: 0.0003438619911670685\n",
      "[248] train loss (mean of batch losses): 0.0002849253586360386\n",
      "[248] val loss: 0.0003655005693435669\n",
      "[249] train loss (mean of batch losses): 0.0002817674168518611\n",
      "[249] val loss: 0.0003370959877967834\n",
      "[250] train loss (mean of batch losses): 0.00027167993954249793\n",
      "[250] val loss: 0.0003590562880039215\n",
      "[251] train loss (mean of batch losses): 0.0002745889638151441\n",
      "[251] val loss: 0.0003350448191165924\n",
      "[252] train loss (mean of batch losses): 0.000275917740379061\n",
      "[252] val loss: 0.0003597931444644928\n",
      "[253] train loss (mean of batch losses): 0.00028789737480027335\n",
      "[253] val loss: 0.00036187824010849\n",
      "[254] train loss (mean of batch losses): 0.00027337747386523654\n",
      "[254] val loss: 0.00034558270573616025\n",
      "[255] train loss (mean of batch losses): 0.0002833307138511113\n",
      "[255] val loss: 0.0003428831219673157\n",
      "[256] train loss (mean of batch losses): 0.0002796411693096161\n",
      "[256] val loss: 0.0003376461923122406\n",
      "[257] train loss (mean of batch losses): 0.000270583262188094\n",
      "[257] val loss: 0.0003343872249126434\n",
      "[258] train loss (mean of batch losses): 0.00027807261773518156\n",
      "[258] val loss: 0.0003513882040977478\n",
      "[259] train loss (mean of batch losses): 0.0002818980600152697\n",
      "[259] val loss: 0.00033958998322486877\n",
      "[260] train loss (mean of batch losses): 0.00028396670477730886\n",
      "[260] val loss: 0.0003430256724357605\n",
      "[261] train loss (mean of batch losses): 0.00027519032571996963\n",
      "[261] val loss: 0.00033361462950706484\n",
      "[262] train loss (mean of batch losses): 0.00026762413808277676\n",
      "[262] val loss: 0.0003268774449825287\n",
      "[263] train loss (mean of batch losses): 0.0002655136627810342\n",
      "[263] val loss: 0.00032813016176223755\n",
      "[264] train loss (mean of batch losses): 0.0002649466472012656\n",
      "[264] val loss: 0.0003332362771034241\n",
      "[265] train loss (mean of batch losses): 0.0002809325269290379\n",
      "[265] val loss: 0.00032853759527206423\n",
      "[266] train loss (mean of batch losses): 0.0002685150453022548\n",
      "[266] val loss: 0.0003376055061817169\n",
      "[267] train loss (mean of batch losses): 0.00026555218441145763\n",
      "[267] val loss: 0.0003289026319980621\n",
      "[268] train loss (mean of batch losses): 0.00026777852518217903\n",
      "[268] val loss: 0.00032153656482696534\n",
      "[269] train loss (mean of batch losses): 0.0002690095365047455\n",
      "[269] val loss: 0.00032823222279548646\n",
      "[270] train loss (mean of batch losses): 0.0002923150054046086\n",
      "[270] val loss: 0.00034609150886535646\n",
      "[271] train loss (mean of batch losses): 0.0002753593495913914\n",
      "[271] val loss: 0.00032794365286827087\n",
      "[272] train loss (mean of batch losses): 0.00028035065957478114\n",
      "[272] val loss: 0.0003216749370098114\n",
      "[273] train loss (mean of batch losses): 0.0002665660245077951\n",
      "[273] val loss: 0.00034193952679634096\n",
      "[274] train loss (mean of batch losses): 0.00027747873323304313\n",
      "[274] val loss: 0.00032557536959648134\n",
      "[275] train loss (mean of batch losses): 0.00027430399145398824\n",
      "[275] val loss: 0.0003533907473087311\n",
      "[276] train loss (mean of batch losses): 0.00028007232546806333\n",
      "[276] val loss: 0.0003284600615501404\n",
      "[277] train loss (mean of batch losses): 0.00025525258566652025\n",
      "[277] val loss: 0.0003117531895637512\n",
      "[278] train loss (mean of batch losses): 0.0002485840107713427\n",
      "[278] val loss: 0.0003191047489643097\n",
      "[279] train loss (mean of batch losses): 0.00026371522290366035\n",
      "[279] val loss: 0.0003226767301559448\n",
      "[280] train loss (mean of batch losses): 0.00025523729154041834\n",
      "[280] val loss: 0.00031554697155952455\n",
      "[281] train loss (mean of batch losses): 0.00025274478452546256\n",
      "[281] val loss: 0.00030774515271186827\n",
      "[282] train loss (mean of batch losses): 0.00025434100117002214\n",
      "[282] val loss: 0.0003350510537624359\n",
      "[283] train loss (mean of batch losses): 0.00026199035048484804\n",
      "[283] val loss: 0.0003841089963912964\n",
      "[284] train loss (mean of batch losses): 0.0002771588580948966\n",
      "[284] val loss: 0.00033452953696250915\n",
      "[285] train loss (mean of batch losses): 0.0002580703658717019\n",
      "[285] val loss: 0.0003450659990310669\n",
      "[286] train loss (mean of batch losses): 0.0002533898634569986\n",
      "[286] val loss: 0.0003235797643661499\n",
      "[287] train loss (mean of batch losses): 0.0002487302107470376\n",
      "[287] val loss: 0.0003061146378517151\n",
      "[288] train loss (mean of batch losses): 0.00024221235258238655\n",
      "[288] val loss: 0.00030603031516075133\n",
      "[289] train loss (mean of batch losses): 0.0002470003928456988\n",
      "[289] val loss: 0.0003187680184841156\n",
      "[290] train loss (mean of batch losses): 0.000252729435477938\n",
      "[290] val loss: 0.00029932577013969423\n",
      "[291] train loss (mean of batch losses): 0.00024392290115356446\n",
      "[291] val loss: 0.00030300644636154173\n",
      "[292] train loss (mean of batch losses): 0.0002568603456020355\n",
      "[292] val loss: 0.000391988867521286\n",
      "[293] train loss (mean of batch losses): 0.0002959890842437744\n",
      "[293] val loss: 0.0003863830864429474\n",
      "[294] train loss (mean of batch losses): 0.00026665323546954566\n",
      "[294] val loss: 0.00029413020610809324\n",
      "[295] train loss (mean of batch losses): 0.0002454252941267831\n",
      "[295] val loss: 0.0003008810997009277\n",
      "[296] train loss (mean of batch losses): 0.0002404813651527677\n",
      "[296] val loss: 0.0003120197117328644\n",
      "[297] train loss (mean of batch losses): 0.00026139789649418424\n",
      "[297] val loss: 0.0004129313170909882\n",
      "[298] train loss (mean of batch losses): 0.0002743383458682469\n",
      "[298] val loss: 0.00032409186363220214\n",
      "[299] train loss (mean of batch losses): 0.00025543978469712395\n",
      "[299] val loss: 0.00031422813534736634\n",
      "[300] train loss (mean of batch losses): 0.00025006439770971025\n",
      "[300] val loss: 0.0003200137495994568\n",
      "[301] train loss (mean of batch losses): 0.0002498654544353485\n",
      "[301] val loss: 0.0002996852934360504\n",
      "[302] train loss (mean of batch losses): 0.00024118857383728027\n",
      "[302] val loss: 0.00030709793567657473\n",
      "[303] train loss (mean of batch losses): 0.0002468927655901228\n",
      "[303] val loss: 0.00030628104209899903\n",
      "[304] train loss (mean of batch losses): 0.00024855034947395325\n",
      "[304] val loss: 0.000308940988779068\n",
      "[305] train loss (mean of batch losses): 0.0002502984004361289\n",
      "[305] val loss: 0.00029503613114356995\n",
      "Epoch   305: reducing learning rate of group 0 to 2.1357e-03.\n",
      "[306] train loss (mean of batch losses): 0.00024988645144871305\n",
      "[306] val loss: 0.0003022387206554413\n",
      "[307] train loss (mean of batch losses): 0.00024957900983946663\n",
      "[307] val loss: 0.0002910992741584778\n",
      "[308] train loss (mean of batch losses): 0.0002380700102874211\n",
      "[308] val loss: 0.0002925687670707703\n",
      "[309] train loss (mean of batch losses): 0.00024906727245875766\n",
      "[309] val loss: 0.0002938913583755493\n",
      "[310] train loss (mean of batch losses): 0.0002444009014538356\n",
      "[310] val loss: 0.0002933598279953003\n",
      "[311] train loss (mean of batch losses): 0.00023621485999652319\n",
      "[311] val loss: 0.0002870133578777313\n",
      "[312] train loss (mean of batch losses): 0.0002391258682523455\n",
      "[312] val loss: 0.0003058329701423645\n",
      "[313] train loss (mean of batch losses): 0.000246229157277516\n",
      "[313] val loss: 0.00028928107619285585\n",
      "[314] train loss (mean of batch losses): 0.00023443880251475743\n",
      "[314] val loss: 0.00030020534992218016\n",
      "[315] train loss (mean of batch losses): 0.0002388287501675742\n",
      "[315] val loss: 0.0002962917566299438\n",
      "[316] train loss (mean of batch losses): 0.00022876890003681183\n",
      "[316] val loss: 0.00028581268191337584\n",
      "[317] train loss (mean of batch losses): 0.00023037530950137546\n",
      "[317] val loss: 0.000291084361076355\n",
      "[318] train loss (mean of batch losses): 0.00024177102446556092\n",
      "[318] val loss: 0.00029657090902328494\n",
      "[319] train loss (mean of batch losses): 0.00024055614641734532\n",
      "[319] val loss: 0.00029267764687538147\n",
      "[320] train loss (mean of batch losses): 0.00023318316681044442\n",
      "[320] val loss: 0.0002944137513637543\n",
      "[321] train loss (mean of batch losses): 0.0002326986048902784\n",
      "[321] val loss: 0.00029718630313873293\n",
      "[322] train loss (mean of batch losses): 0.00023176709158079966\n",
      "[322] val loss: 0.0002857749044895172\n",
      "[323] train loss (mean of batch losses): 0.00023550635746547155\n",
      "[323] val loss: 0.00029343891143798827\n",
      "[324] train loss (mean of batch losses): 0.00023902301618031093\n",
      "[324] val loss: 0.0002999550938606262\n",
      "[325] train loss (mean of batch losses): 0.00023354394095284598\n",
      "[325] val loss: 0.00031153054833412173\n",
      "[326] train loss (mean of batch losses): 0.0002708371477467673\n",
      "[326] val loss: 0.0002957596480846405\n",
      "[327] train loss (mean of batch losses): 0.00023667891877038138\n",
      "[327] val loss: 0.0002900043547153473\n",
      "[328] train loss (mean of batch losses): 0.0002293669547353472\n",
      "[328] val loss: 0.0002799994468688965\n",
      "[329] train loss (mean of batch losses): 0.00024199807643890381\n",
      "[329] val loss: 0.00027625572681427\n",
      "[330] train loss (mean of batch losses): 0.00025154250604765755\n",
      "[330] val loss: 0.00028080774545669556\n",
      "[331] train loss (mean of batch losses): 0.00022802541383675168\n",
      "[331] val loss: 0.0002995289623737335\n",
      "[332] train loss (mean of batch losses): 0.00022815132098538536\n",
      "[332] val loss: 0.00028392274379730227\n",
      "[333] train loss (mean of batch losses): 0.00023113494770867483\n",
      "[333] val loss: 0.0003017570376396179\n",
      "[334] train loss (mean of batch losses): 0.0002339818503175463\n",
      "[334] val loss: 0.0002969592571258545\n",
      "[335] train loss (mean of batch losses): 0.0002357609910624368\n",
      "[335] val loss: 0.0002811495304107666\n",
      "[336] train loss (mean of batch losses): 0.0002290304720401764\n",
      "[336] val loss: 0.00028059701323509216\n",
      "[337] train loss (mean of batch losses): 0.00022222770963396346\n",
      "[337] val loss: 0.00028065404295921325\n",
      "[338] train loss (mean of batch losses): 0.00022181450894900732\n",
      "[338] val loss: 0.00028154633939266206\n",
      "[339] train loss (mean of batch losses): 0.00022935174788747516\n",
      "[339] val loss: 0.0002781334847211838\n",
      "[340] train loss (mean of batch losses): 0.00022490067481994628\n",
      "[340] val loss: 0.0002926902115345001\n",
      "Epoch   340: reducing learning rate of group 0 to 1.6018e-03.\n",
      "[341] train loss (mean of batch losses): 0.00022832543509347097\n",
      "[341] val loss: 0.0002829814016819\n",
      "[342] train loss (mean of batch losses): 0.0002322850125176566\n",
      "[342] val loss: 0.0002729253828525543\n",
      "[343] train loss (mean of batch losses): 0.0002245091131755284\n",
      "[343] val loss: 0.00027247075736522677\n",
      "[344] train loss (mean of batch losses): 0.0002175933301448822\n",
      "[344] val loss: 0.0002754133373498917\n",
      "[345] train loss (mean of batch losses): 0.00022255536232675824\n",
      "[345] val loss: 0.00027775717079639434\n",
      "[346] train loss (mean of batch losses): 0.0002324487124170576\n",
      "[346] val loss: 0.00028436910510063173\n",
      "[347] train loss (mean of batch losses): 0.00023207333173070634\n",
      "[347] val loss: 0.0002762480318546295\n",
      "[348] train loss (mean of batch losses): 0.00023949122514043536\n",
      "[348] val loss: 0.00028303066492080687\n",
      "[349] train loss (mean of batch losses): 0.0002379710946764265\n",
      "[349] val loss: 0.0002818798899650574\n",
      "[350] train loss (mean of batch losses): 0.00023232038872582572\n",
      "[350] val loss: 0.00028603331446647645\n",
      "[351] train loss (mean of batch losses): 0.00022367741465568542\n",
      "[351] val loss: 0.0002733447164297104\n",
      "[352] train loss (mean of batch losses): 0.00022604414565222604\n",
      "[352] val loss: 0.0002775951862335205\n",
      "[353] train loss (mean of batch losses): 0.00022630482145718166\n",
      "[353] val loss: 0.00028300886750221255\n",
      "[354] train loss (mean of batch losses): 0.0002201117375067302\n",
      "[354] val loss: 0.0002757956087589264\n",
      "Epoch   354: reducing learning rate of group 0 to 1.2014e-03.\n",
      "[355] train loss (mean of batch losses): 0.00022046966467584883\n",
      "[355] val loss: 0.0002712937146425247\n",
      "[356] train loss (mean of batch losses): 0.00021672747305461339\n",
      "[356] val loss: 0.00027058422565460203\n",
      "[357] train loss (mean of batch losses): 0.00022713980078697203\n",
      "[357] val loss: 0.0002718390822410584\n",
      "[358] train loss (mean of batch losses): 0.00021774020961352756\n",
      "[358] val loss: 0.00027382572293281555\n",
      "[359] train loss (mean of batch losses): 0.0002249557248183659\n",
      "[359] val loss: 0.0002692026883363724\n",
      "[360] train loss (mean of batch losses): 0.00022096915755953107\n",
      "[360] val loss: 0.0002756537705659866\n",
      "[361] train loss (mean of batch losses): 0.00022214927162442889\n",
      "[361] val loss: 0.00027364569306373595\n",
      "[362] train loss (mean of batch losses): 0.00022346581050327845\n",
      "[362] val loss: 0.0002697269469499588\n",
      "[363] train loss (mean of batch losses): 0.00022284629770687647\n",
      "[363] val loss: 0.00027663931250572206\n",
      "[364] train loss (mean of batch losses): 0.00021967915552003044\n",
      "[364] val loss: 0.0002691202461719513\n",
      "[365] train loss (mean of batch losses): 0.00021235570226396834\n",
      "[365] val loss: 0.00026933634877204896\n",
      "[366] train loss (mean of batch losses): 0.00022020166260855537\n",
      "[366] val loss: 0.000271154648065567\n",
      "[367] train loss (mean of batch losses): 0.00022100563645362854\n",
      "[367] val loss: 0.00027226066887378695\n",
      "[368] train loss (mean of batch losses): 0.00022297286646706717\n",
      "[368] val loss: 0.00027709822654724123\n",
      "[369] train loss (mean of batch losses): 0.00022711277433804102\n",
      "[369] val loss: 0.00026953113079071045\n",
      "[370] train loss (mean of batch losses): 0.00023293804356030055\n",
      "[370] val loss: 0.00026852426528930666\n",
      "[371] train loss (mean of batch losses): 0.00022914611271449497\n",
      "[371] val loss: 0.0002766460657119751\n",
      "[372] train loss (mean of batch losses): 0.0002200718147414071\n",
      "[372] val loss: 0.0002674145311117172\n",
      "[373] train loss (mean of batch losses): 0.00022314625552722386\n",
      "[373] val loss: 0.00027256937325000763\n",
      "[374] train loss (mean of batch losses): 0.00023303207755088806\n",
      "[374] val loss: 0.0002676434487104416\n",
      "[375] train loss (mean of batch losses): 0.0002188826893057142\n",
      "[375] val loss: 0.0002674283623695374\n",
      "[376] train loss (mean of batch losses): 0.0002202718164239611\n",
      "[376] val loss: 0.000269325852394104\n",
      "[377] train loss (mean of batch losses): 0.0002506162260259901\n",
      "[377] val loss: 0.00027832010090351106\n",
      "[378] train loss (mean of batch losses): 0.0002198868508849825\n",
      "[378] val loss: 0.0002761790990829468\n",
      "[379] train loss (mean of batch losses): 0.00022014134100505284\n",
      "[379] val loss: 0.00026781705021858217\n",
      "[380] train loss (mean of batch losses): 0.0002242050554071154\n",
      "[380] val loss: 0.00027338646054267884\n",
      "[381] train loss (mean of batch losses): 0.000220152234179633\n",
      "[381] val loss: 0.00027291342318058016\n",
      "[382] train loss (mean of batch losses): 0.0002264584447656359\n",
      "[382] val loss: 0.0002757394939661026\n",
      "[383] train loss (mean of batch losses): 0.00022339555195399692\n",
      "[383] val loss: 0.0002752448171377182\n",
      "Epoch   383: reducing learning rate of group 0 to 9.0102e-04.\n",
      "[384] train loss (mean of batch losses): 0.0002153419417994363\n",
      "[384] val loss: 0.00026720304489135743\n",
      "[385] train loss (mean of batch losses): 0.00021039361570562634\n",
      "[385] val loss: 0.0002665865689516068\n",
      "[386] train loss (mean of batch losses): 0.0002115423551627568\n",
      "[386] val loss: 0.0002664836078882217\n",
      "[387] train loss (mean of batch losses): 0.00022164351855005537\n",
      "[387] val loss: 0.0002655634373426437\n",
      "[388] train loss (mean of batch losses): 0.00022100165826933725\n",
      "[388] val loss: 0.0002729773879051208\n",
      "[389] train loss (mean of batch losses): 0.00021792045150484358\n",
      "[389] val loss: 0.000263936921954155\n",
      "[390] train loss (mean of batch losses): 0.00021743326357432773\n",
      "[390] val loss: 0.00026485361754894255\n",
      "[391] train loss (mean of batch losses): 0.00020960089904921395\n",
      "[391] val loss: 0.00026405536234378813\n",
      "[392] train loss (mean of batch losses): 0.00020714032862867627\n",
      "[392] val loss: 0.0002643712610006332\n",
      "[393] train loss (mean of batch losses): 0.0002171591545854296\n",
      "[393] val loss: 0.0002790696918964386\n",
      "[394] train loss (mean of batch losses): 0.00022670306733676366\n",
      "[394] val loss: 0.00026896198987960815\n",
      "[395] train loss (mean of batch losses): 0.00021549666098185947\n",
      "[395] val loss: 0.00026266511678695677\n",
      "[396] train loss (mean of batch losses): 0.00021350796222686767\n",
      "[396] val loss: 0.00026319950520992276\n",
      "[397] train loss (mean of batch losses): 0.00021797647305897303\n",
      "[397] val loss: 0.0002631819665431976\n",
      "[398] train loss (mean of batch losses): 0.0002061957210302353\n",
      "[398] val loss: 0.0002641619712114334\n",
      "[399] train loss (mean of batch losses): 0.0002108831865446908\n",
      "[399] val loss: 0.000264748814702034\n",
      "[400] train loss (mean of batch losses): 0.0002125017353466579\n",
      "[400] val loss: 0.00026505787074565887\n",
      "Training finished\n",
      "Generating results...\n",
      "Experiment finished\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 3 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Working directory : {}\".format(cwd))\n",
    "\n",
    "dataset_path = build_dataset_path(**cfg['dataset_params'])\n",
    "dataset_path = os.path.join(cwd, 'datasets', dataset_path)\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "loaders = build_loaders(datasets, cfg['bs'], cfg['num_workers'])\n",
    "\n",
    "results, run = run_training(loaders=loaders,\n",
    "                            cfg=cfg,\n",
    "                            **cfg)\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e3998f3",
   "metadata": {},
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63332785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952669b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = set(datasets.keys()) | set(results.keys())\n",
    "base_dir = os.getcwd()\n",
    "setup_dirs(folder_names, base_dir)\n",
    "dump_results(datasets, results, base_dir, run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}