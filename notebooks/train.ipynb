{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597c516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c22c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.utils import get_original_cwd\n",
    "from omegaconf import DictConfig\n",
    "from config import run_training\n",
    "from data import build_loaders\n",
    "from plot import setup_dirs, dump_results\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def build_dataset_path(seed, n_samples, dim, eps, curv, transform_dim):\n",
    "    template = 'dim={},n_samples={},eps={},transform_dim={},curv={},seed={}.pkl'\n",
    "    return template.format(int(dim), int(n_samples), eps, transform_dim, curv, seed)\n",
    "\n",
    "\n",
    "def train(cfg: DictConfig) -> None:\n",
    "    cwd = os.getcwd()\n",
    "    print(\"Working directory : {}\".format(cwd))\n",
    "\n",
    "    dataset_path = build_dataset_path(**cfg['dataset_params'])\n",
    "    dataset_path = os.path.join(cwd, 'datasets', dataset_path)\n",
    "\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        datasets = pickle.load(f)\n",
    "    loaders = build_loaders(datasets, cfg['bs'], cfg['num_workers'])\n",
    "\n",
    "    results, run = run_training(loaders=loaders,\n",
    "                                cfg=cfg,\n",
    "                                **cfg)\n",
    "    folder_names = set(datasets.keys()) | set(results.keys())\n",
    "    base_dir = os.getcwd()\n",
    "    setup_dirs(folder_names, base_dir)\n",
    "    dump_results(datasets, results, base_dir, run)\n",
    "    if run is not None:\n",
    "        run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e1a49",
   "metadata": {},
   "source": [
    "# build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc056a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 777, 'n_samples': 100000.0, 'dim': 2, 'eps': 0.01, 'curv': 1, 'transform_dim': '${.dim}', 'datasets_folder': 'datasets'}\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "from data import build_datasets\n",
    "\n",
    "base_overrides = ['datasets_folder=datasets']\n",
    "# multirun = [['dim=i']]\n",
    "\n",
    "# for additional_ovrs in multirun:\n",
    "#     bo_copy = set(base_overrides)\n",
    "#     bo_copy.extend(additional_ovrs)\n",
    "\n",
    "with initialize(config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"dataset_generation.yaml\", overrides=base_overrides)\n",
    "    print(cfg)\n",
    "    build_datasets(**cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051046f",
   "metadata": {},
   "source": [
    "# run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9c373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory : /scidatalg/jkoc/hyper-dl\n",
      "https://app.neptune.ai/jkociniak/hyperbolic-dist-pred/e/HYPERDIST-2704\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Found device: cuda\n",
      "[1] train loss (mean of batch losses): 1.8063308995383127\n",
      "[1] val loss: 0.1921956802368164\n",
      "[2] train loss (mean of batch losses): 0.1436576313018799\n",
      "[2] val loss: 0.13269782819747925\n",
      "[3] train loss (mean of batch losses): 0.1218020978655134\n",
      "[3] val loss: 0.12143896350860596\n",
      "[4] train loss (mean of batch losses): 0.11299987373352051\n",
      "[4] val loss: 0.11397585487365723\n",
      "[5] train loss (mean of batch losses): 0.10662208243778774\n",
      "[5] val loss: 0.10705919742584229\n",
      "[6] train loss (mean of batch losses): 0.09974978813443865\n",
      "[6] val loss: 0.1006148281097412\n",
      "[7] train loss (mean of batch losses): 0.09423584240504673\n",
      "[7] val loss: 0.09569735169410705\n",
      "[8] train loss (mean of batch losses): 0.08969459400177002\n",
      "[8] val loss: 0.0918939920425415\n",
      "[9] train loss (mean of batch losses): 0.08590523369652885\n",
      "[9] val loss: 0.08734160585403443\n",
      "[10] train loss (mean of batch losses): 0.08196835170473371\n",
      "[10] val loss: 0.08452352609634399\n",
      "[11] train loss (mean of batch losses): 0.07849778300694057\n",
      "[11] val loss: 0.07890708429813385\n",
      "[12] train loss (mean of batch losses): 0.07516389758246285\n",
      "[12] val loss: 0.07617840657234191\n",
      "[13] train loss (mean of batch losses): 0.07232218833650861\n",
      "[13] val loss: 0.07378631148338317\n",
      "[14] train loss (mean of batch losses): 0.06908402859824045\n",
      "[14] val loss: 0.07032131161689759\n",
      "[15] train loss (mean of batch losses): 0.06656748446055821\n",
      "[15] val loss: 0.06821721827983857\n",
      "[16] train loss (mean of batch losses): 0.06440648615019662\n",
      "[16] val loss: 0.0649442218542099\n",
      "[17] train loss (mean of batch losses): 0.06210579328536987\n",
      "[17] val loss: 0.06421144700050355\n",
      "[18] train loss (mean of batch losses): 0.05996764744349888\n",
      "[18] val loss: 0.061290030765533444\n",
      "[19] train loss (mean of batch losses): 0.0585779116494315\n",
      "[19] val loss: 0.05871188607215881\n",
      "[20] train loss (mean of batch losses): 0.05710158839225769\n",
      "[20] val loss: 0.06113883538246155\n",
      "[21] train loss (mean of batch losses): 0.055237723357336864\n",
      "[21] val loss: 0.05635338559150696\n",
      "[22] train loss (mean of batch losses): 0.05360380600520543\n",
      "[22] val loss: 0.06213653659820557\n",
      "[23] train loss (mean of batch losses): 0.052837461383002146\n",
      "[23] val loss: 0.05398417992591858\n",
      "[24] train loss (mean of batch losses): 0.05102547785214016\n",
      "[24] val loss: 0.053084698390960694\n",
      "[25] train loss (mean of batch losses): 0.05013239682742528\n",
      "[25] val loss: 0.050253739500045774\n",
      "[26] train loss (mean of batch losses): 0.04951201126916068\n",
      "[26] val loss: 0.04914449017047882\n",
      "[27] train loss (mean of batch losses): 0.04748796882629395\n",
      "[27] val loss: 0.04878935434818268\n",
      "[28] train loss (mean of batch losses): 0.04713829906327384\n",
      "[28] val loss: 0.047229281425476076\n",
      "[29] train loss (mean of batch losses): 0.04564419172831944\n",
      "[29] val loss: 0.046219528985023496\n",
      "[30] train loss (mean of batch losses): 0.045405539982659476\n",
      "[30] val loss: 0.04563649935722351\n",
      "[31] train loss (mean of batch losses): 0.044037899099077495\n",
      "[31] val loss: 0.045484035682678225\n",
      "[32] train loss (mean of batch losses): 0.04314670053209577\n",
      "[32] val loss: 0.04356760082244873\n",
      "[33] train loss (mean of batch losses): 0.04275374936376299\n",
      "[33] val loss: 0.04389782168865204\n",
      "[34] train loss (mean of batch losses): 0.041983783265522545\n",
      "[34] val loss: 0.041367152404785154\n",
      "[35] train loss (mean of batch losses): 0.04133557336671012\n",
      "[35] val loss: 0.04219647988080978\n",
      "[36] train loss (mean of batch losses): 0.04056343684196472\n",
      "[36] val loss: 0.041958087015151975\n",
      "[37] train loss (mean of batch losses): 0.03992310869353158\n",
      "[37] val loss: 0.041809281158447266\n",
      "[38] train loss (mean of batch losses): 0.03901904642241342\n",
      "[38] val loss: 0.040252581703662874\n",
      "[39] train loss (mean of batch losses): 0.038656232738494875\n",
      "[39] val loss: 0.03816127173900604\n",
      "[40] train loss (mean of batch losses): 0.038098318661962234\n",
      "[40] val loss: 0.03913789162635803\n",
      "[41] train loss (mean of batch losses): 0.037691899473326546\n",
      "[41] val loss: 0.04344331955909729\n",
      "[42] train loss (mean of batch losses): 0.03767519333021981\n",
      "[42] val loss: 0.03815147874355316\n",
      "[43] train loss (mean of batch losses): 0.03666126338413783\n",
      "[43] val loss: 0.04079286501407623\n",
      "[44] train loss (mean of batch losses): 0.03664800681386675\n",
      "[44] val loss: 0.03582905839681625\n",
      "[45] train loss (mean of batch losses): 0.036219310876301355\n",
      "[45] val loss: 0.03526313998699188\n",
      "[46] train loss (mean of batch losses): 0.03493556033543178\n",
      "[46] val loss: 0.03624221403598785\n",
      "[47] train loss (mean of batch losses): 0.03486567600114005\n",
      "[47] val loss: 0.043130687141418454\n",
      "[48] train loss (mean of batch losses): 0.034354399997847423\n",
      "[48] val loss: 0.035543960571289064\n",
      "[49] train loss (mean of batch losses): 0.03459406476702009\n",
      "[49] val loss: 0.03826359490156174\n",
      "[50] train loss (mean of batch losses): 0.0334794018472944\n",
      "[50] val loss: 0.0353767546415329\n",
      "[51] train loss (mean of batch losses): 0.03342236672469548\n",
      "[51] val loss: 0.03657919585704803\n",
      "[52] train loss (mean of batch losses): 0.03417355817386082\n",
      "[52] val loss: 0.034935774874687194\n",
      "[53] train loss (mean of batch losses): 0.03288350648539407\n",
      "[53] val loss: 0.035641302907466886\n",
      "[54] train loss (mean of batch losses): 0.031475577425956726\n",
      "[54] val loss: 0.03078166217803955\n",
      "[55] train loss (mean of batch losses): 0.032068741106987\n",
      "[55] val loss: 0.03149952673912048\n",
      "[56] train loss (mean of batch losses): 0.032279631096976145\n",
      "[56] val loss: 0.030400798308849335\n",
      "[57] train loss (mean of batch losses): 0.03186823195729937\n",
      "[57] val loss: 0.036446818733215335\n",
      "[58] train loss (mean of batch losses): 0.031021408469336372\n",
      "[58] val loss: 0.030755400693416597\n",
      "[59] train loss (mean of batch losses): 0.030467632055282592\n",
      "[59] val loss: 0.03175231328010559\n",
      "[60] train loss (mean of batch losses): 0.031595855597087316\n",
      "[60] val loss: 0.03272654185295105\n",
      "[61] train loss (mean of batch losses): 0.030580494025775364\n",
      "[61] val loss: 0.040763918256759644\n",
      "[62] train loss (mean of batch losses): 0.031566036684172495\n",
      "[62] val loss: 0.040912221550941465\n",
      "[63] train loss (mean of batch losses): 0.03083840480531965\n",
      "[63] val loss: 0.029667970299720764\n",
      "[64] train loss (mean of batch losses): 0.029242674711772373\n",
      "[64] val loss: 0.029064957547187804\n",
      "[65] train loss (mean of batch losses): 0.029340605671065195\n",
      "[65] val loss: 0.030407300877571104\n",
      "[66] train loss (mean of batch losses): 0.02911225722857884\n",
      "[66] val loss: 0.030926478946208955\n",
      "[67] train loss (mean of batch losses): 0.028476123636109487\n",
      "[67] val loss: 0.029051013588905335\n",
      "[68] train loss (mean of batch losses): 0.0281793345281056\n",
      "[68] val loss: 0.028213127219676972\n",
      "[69] train loss (mean of batch losses): 0.02926273569379534\n",
      "[69] val loss: 0.031099816763401032\n",
      "[70] train loss (mean of batch losses): 0.02880566874231611\n",
      "[70] val loss: 0.02847835590839386\n",
      "[71] train loss (mean of batch losses): 0.02785128855705261\n",
      "[71] val loss: 0.0328837104678154\n",
      "[72] train loss (mean of batch losses): 0.028832233827454704\n",
      "[72] val loss: 0.031348225271701814\n",
      "[73] train loss (mean of batch losses): 0.02727770428657532\n",
      "[73] val loss: 0.04134783544540405\n",
      "[74] train loss (mean of batch losses): 0.028041802770750864\n",
      "[74] val loss: 0.03063306543827057\n",
      "[75] train loss (mean of batch losses): 0.02768908202648163\n",
      "[75] val loss: 0.028979515159130095\n",
      "[76] train loss (mean of batch losses): 0.02731727637563433\n",
      "[76] val loss: 0.026524873626232147\n",
      "[77] train loss (mean of batch losses): 0.027184674126761302\n",
      "[77] val loss: 0.026210753870010376\n",
      "[78] train loss (mean of batch losses): 0.02656515453202384\n",
      "[78] val loss: 0.029175293362140656\n",
      "[79] train loss (mean of batch losses): 0.02660964263507298\n",
      "[79] val loss: 0.025196609330177307\n",
      "[80] train loss (mean of batch losses): 0.025791748554365976\n",
      "[80] val loss: 0.027115718674659728\n",
      "[81] train loss (mean of batch losses): 0.025961996378217424\n",
      "[81] val loss: 0.03645526738166809\n",
      "[82] train loss (mean of batch losses): 0.027588791002546038\n",
      "[82] val loss: 0.026803711676597595\n",
      "[83] train loss (mean of batch losses): 0.026792698873792375\n",
      "[83] val loss: 0.02979932585954666\n",
      "[84] train loss (mean of batch losses): 0.02729624936580658\n",
      "[84] val loss: 0.034390903985500335\n",
      "[85] train loss (mean of batch losses): 0.027688429038865224\n",
      "[85] val loss: 0.03886625168323517\n",
      "[86] train loss (mean of batch losses): 0.025432857305662974\n",
      "[86] val loss: 0.04173517653942108\n",
      "[87] train loss (mean of batch losses): 0.02648192058631352\n",
      "[87] val loss: 0.02544211469888687\n",
      "[88] train loss (mean of batch losses): 0.0255061665058136\n",
      "[88] val loss: 0.023722133457660676\n",
      "[89] train loss (mean of batch losses): 0.025485396599769592\n",
      "[89] val loss: 0.024510076451301575\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "overrides = [\n",
    "    'epochs=200',\n",
    "    'model.head.hidden_dims=[320,320,320]'\n",
    "]\n",
    "\n",
    "with initialize(config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config.yaml\", overrides=overrides)\n",
    "    train(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56234ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: DoubleInputHyperbolicFFN\n",
      "input_dim: 2\n",
      "hidden_dims: []\n",
      "output_dim: 2\n",
      "bias: False\n",
      "curv: 1\n",
      "activation: None\n",
      "skips: False"
     ]
    }
   ],
   "source": [
    "!cat conf/layers/2input_hFFN.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e3998f3",
   "metadata": {},
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63332785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952669b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
